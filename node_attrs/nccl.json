{
 "PRed":[
  {
   "PR":{
    "__lazy_json__":"pr_json/367072026.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"Version",
    "migrator_version":0,
    "version":"2.5.7-1"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR":{
    "__lazy_json__":"pr_json/394864347.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"Version",
    "migrator_version":0,
    "version":"2.6.4-1"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  }
 ],
 "bad":false,
 "conda-forge.yml":{},
 "feedstock_name":"nccl",
 "hash_type":"sha256",
 "meta_yaml":{
  "about":{
   "description":"The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
   "dev_url":"https://github.com/NVIDIA/nccl",
   "doc_url":"https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
   "home":"https://developer.nvidia.com/nccl",
   "license":"BSD-3-Clause",
   "license_family":"BSD",
   "license_file":"LICENSE.txt",
   "summary":"Optimized primitives for collective multi-GPU communication"
  },
  "build":{
   "number":"0",
   "run_exports":[
    "nccl",
    "nccl",
    "nccl"
   ],
   "skip":true
  },
  "extra":{
   "recipe-maintainers":[
    "jakirkham",
    "jakirkham",
    "jakirkham"
   ]
  },
  "package":{
   "name":"nccl",
   "version":"2.6.4.1"
  },
  "requirements":{
   "build":[
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make"
   ]
  },
  "source":{
   "sha256":"ed8c9dfd40e013003923ae006787b1a30d3cb363b47d2e4307eaa2624ebba2ba",
   "url":"https://github.com/NVIDIA/nccl/archive/v2.6.4-1.tar.gz"
  },
  "test":{
   "commands":[
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\""
   ]
  }
 },
 "name":"nccl",
 "new_version":"2.6.4-1",
 "pinning_version":"2020.03.24",
 "raw_meta_yaml":"{% set name = \"nccl\" %}\n{% set version = \"2.6.4-1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version|replace(\"-\", \".\") }}\n\nsource:\n  url: https://github.com/NVIDIA/nccl/archive/v{{ version }}.tar.gz\n  sha256: ed8c9dfd40e013003923ae006787b1a30d3cb363b47d2e4307eaa2624ebba2ba\n\nbuild:\n  number: 0\n  skip: true  # [(not linux64) or (cuda_compiler_version == \"None\")]\n  run_exports:\n    # xref: https://github.com/NVIDIA/nccl/issues/218\n    - {{ pin_subpackage(name, max_pin=\"x\") }}\n\nrequirements:\n  build:\n    - {{ compiler(\"c\") }}\n    - {{ compiler(\"cxx\") }}\n    - {{ compiler(\"cuda\") }}\n    - make\n\ntest:\n  commands:\n    - test -f \"${PREFIX}/include/nccl.h\"\n    - test -f \"${PREFIX}/lib/libnccl.so\"\n    - test -f \"${PREFIX}/lib/libnccl_static.a\"\n\nabout:\n  home: https://developer.nvidia.com/nccl\n  license: BSD-3-Clause\n  license_family: BSD\n  license_file: LICENSE.txt\n  summary: Optimized primitives for collective multi-GPU communication\n\n  description: |\n    The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\n    and multi-node collective communication primitives that are performance\n    optimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\n    all-reduce, broadcast, reduce, reduce-scatter, that are optimized to\n    achieve high bandwidth over PCIe and NVLink high-speed interconnect.\n\n  doc_url: https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html\n  dev_url: https://github.com/NVIDIA/nccl\n\nextra:\n  recipe-maintainers:\n    - jakirkham\n",
 "req":{
  "__set__":true,
  "elements":[
   "c_compiler_stub",
   "cuda_compiler_stub",
   "cxx_compiler_stub",
   "make"
  ]
 },
 "requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[]
  },
  "run":{
   "__set__":true,
   "elements":[]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "smithy_version":"No azure token. Create a token and\nput it in ~/.conda-smithy/azure.token\n3.6.15",
 "strong_exports":false,
 "total_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[]
  },
  "run":{
   "__set__":true,
   "elements":[]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "url":"https://github.com/NVIDIA/nccl/archive/v2.4.6-1.tar.gz",
 "version":"2.6.4.1"
}